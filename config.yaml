# Configuration for AI Video Generation Pipeline

# SDXL LoRA Configuration
sdxl:
  model_path: "stabilityai/stable-diffusion-xl-base-1.0"  # HuggingFace model ID or local path
  lora_path: ""  # Path to your LoRA weights (.safetensors file or directory)
  # Examples:
  # lora_path: "/path/to/your/lora.safetensors"
  # lora_path: "/path/to/lora/directory"  # Directory containing LoRA files
  # lora_path: "username/lora-name"  # HuggingFace LoRA ID
  lora_scale: 1.0  # LoRA strength (0.0-2.0)
  device: "cuda"  # Using CUDA for NVIDIA GB10 GPU
  dtype: "float16"  # Using float16 for memory efficiency (GPU has 119GB VRAM, but float16 is still recommended for speed)
  
# Image-to-Image Configuration
img2img:
  strength: 0.20  # How much to transform the image (0.0-1.0). Lower = more faithful to original (0.15-0.25 for maximum photorealism, minimal transformation)
  guidance_scale: 5.0  # How closely to follow the prompt (1-20, lower for more natural/photorealistic, closer to original)
  num_inference_steps: 75  # More steps = better quality but slower (increased for high quality)
  seed: null  # Set to a number for reproducibility (e.g., 42)
  
# Image Upscaling Configuration
image_upscale:
  method: "realesrgan"  # Options: "realesrgan", "topaz" (if API available)
  scale_factor: 4  # 2x, 4x, etc.
  model_name: "RealESRGAN_x4plus"  # Options: "RealESRGAN_x4plus", "RealESRGAN_x4plus_anime"
  
# Image-to-Video Configuration
# Choose one: "svd" (Stable Video Diffusion - recommended, easier) or "wan" (Wan2.2)
i2v:
  method: "svd"  # Options: "svd", "wan"
  
  # Stable Video Diffusion (SVD) Configuration - Recommended
  svd:
    model_path: "stabilityai/stable-video-diffusion-img2vid"  # HuggingFace model ID
    # Alternative: "stabilityai/stable-video-diffusion-img2vid-xt" for extended version
    num_frames: 25  # Number of frames to generate (SVD supports 14 or 25, using 25 for smoother motion)
    fps: 7  # Frames per second
    motion_bucket_id: 180  # Motion amount (1-255, higher = more motion, increased for standing up action)
    noise_aug_strength: 0.02  # Noise augmentation strength
    decode_chunk_size: 2  # Process frames in chunks for memory efficiency (can increase with 119GB VRAM)
    
  # Wan2.2 I2V Configuration - Requires custom setup
  wan:
    model_path: ""  # Path to Wan2.2 model directory
    # Example: "/path/to/Wan2.2-I2V-A14B"
    # Or HuggingFace: "Wan-AI/Wan2.2-I2V-A14B"
    num_frames: 16
    fps: 8
    motion_bucket_id: 127
    cond_aug: 0.02
    size: "1280*720"  # Output resolution
    offload_model: true  # Offload model to CPU when not in use
    convert_model_dtype: true  # Convert model dtype for compatibility
  
# Video Upscaling Configuration
video_upscale:
  method: "realesrgan"  # Options: "realesrgan", "topaz"
  scale_factor: 2  # Upscale factor for video frames
  frame_interpolation: true  # Add interpolated frames for smoother motion
  
# Output Configuration
output:
  base_dir: "./outputs"
  save_intermediate: true  # Save intermediate results at each step
  image_format: "png"  # Format for saved images
  video_format: "mp4"  # Format for output video
  video_codec: "libx264"  # Video codec
  video_bitrate: "10M"  # Video bitrate

